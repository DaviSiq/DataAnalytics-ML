{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1daea395",
   "metadata": {},
   "source": [
    "### Modelagem preditiva\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cd45b1",
   "metadata": {},
   "source": [
    "### Salvando de volta para csv (com todas as tabelas) para usar exemplos reais"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5b6afc",
   "metadata": {},
   "source": [
    "#### Juntando todas as tabelas em uma sﾃｳ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c858c06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando tabelas do Snowflake...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pacie\\AppData\\Local\\Temp\\ipykernel_17392\\1674283844.py:29: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_orders = pd.read_sql(\"SELECT * FROM orders_refined\", conn)\n",
      "C:\\Users\\pacie\\AppData\\Local\\Temp\\ipykernel_17392\\1674283844.py:30: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_orders_reviews = pd.read_sql(\"SELECT * FROM order_reviews_refined\", conn)\n",
      "C:\\Users\\pacie\\AppData\\Local\\Temp\\ipykernel_17392\\1674283844.py:31: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_order_payments = pd.read_sql(\"SELECT * FROM order_payments_refined\", conn)\n",
      "C:\\Users\\pacie\\AppData\\Local\\Temp\\ipykernel_17392\\1674283844.py:32: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_order_items = pd.read_sql(\"SELECT * FROM order_items_refined\", conn)\n",
      "C:\\Users\\pacie\\AppData\\Local\\Temp\\ipykernel_17392\\1674283844.py:33: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_products = pd.read_sql(\"SELECT * FROM products_refined\", conn)\n",
      "C:\\Users\\pacie\\AppData\\Local\\Temp\\ipykernel_17392\\1674283844.py:34: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_customers = pd.read_sql(\"SELECT * FROM customers_refined\", conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unindo as tabelas...\n",
      "Processo concluﾃｭdo. O arquivo 'olist_full_dataset.csv' foi criado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import snowflake.connector\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "# Carrega as variﾃ｡veis de ambiente\n",
    "env_path = Path('.') / 'env.env'\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "SF_USER = os.getenv(\"SF_USER\")\n",
    "SF_PASSWORD = os.getenv(\"SF_PASSWORD\")\n",
    "SF_ACCOUNT = os.getenv(\"SF_ACCOUNT\")\n",
    "SF_WAREHOUSE = os.getenv(\"SF_WAREHOUSE\")\n",
    "SF_DATABASE = os.getenv(\"SF_DATABASE\")\n",
    "SF_SCHEMA = os.getenv(\"SF_SCHEMA\")\n",
    "\n",
    "# Conecta ao Snowflake\n",
    "conn = snowflake.connector.connect(\n",
    "    user=SF_USER,\n",
    "    password=SF_PASSWORD,\n",
    "    account=SF_ACCOUNT,\n",
    "    warehouse=SF_WAREHOUSE,\n",
    "    database=SF_DATABASE,\n",
    "    schema=SF_SCHEMA\n",
    ")\n",
    "\n",
    "# Carrega as tabelas\n",
    "print(\"Carregando tabelas do Snowflake...\")\n",
    "df_orders = pd.read_sql(\"SELECT * FROM orders_refined\", conn)\n",
    "df_orders_reviews = pd.read_sql(\"SELECT * FROM order_reviews_refined\", conn)\n",
    "df_order_payments = pd.read_sql(\"SELECT * FROM order_payments_refined\", conn)\n",
    "df_order_items = pd.read_sql(\"SELECT * FROM order_items_refined\", conn)\n",
    "df_products = pd.read_sql(\"SELECT * FROM products_refined\", conn)\n",
    "df_customers = pd.read_sql(\"SELECT * FROM customers_refined\", conn)\n",
    "\n",
    "# Fecha a conexﾃ｣o\n",
    "conn.close()\n",
    "\n",
    "# Padroniza os nomes das colunas\n",
    "for df in [df_orders, df_orders_reviews, df_order_payments, df_order_items, df_products, df_customers]:\n",
    "    df.columns = df.columns.str.lower()\n",
    "\n",
    "# Realiza as junﾃｧﾃｵes sequenciais\n",
    "print(\"Unindo as tabelas...\")\n",
    "df_full_orders = df_orders.merge(df_orders_reviews, on='order_id', how='left')\n",
    "df_full_orders = df_full_orders.merge(df_order_payments, on='order_id', how='left')\n",
    "df_full_orders = df_full_orders.merge(df_order_items, on='order_id', how='left')\n",
    "df_full_orders = df_full_orders.merge(df_products, on='product_id', how='left')\n",
    "df_full_orders = df_full_orders.merge(\n",
    "    df_customers[['customer_id', 'customer_state', 'customer_zip_code_prefix']],\n",
    "    on='customer_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Salva o DataFrame completo em um arquivo CSV\n",
    "#print(\"Salvando o DataFrame completo em olist_full_dataset.csv...\")\n",
    "#df_full_orders.to_csv('olist_full_dataset.csv', index=False)\n",
    "\n",
    "print(\"Processo concluﾃｭdo. O arquivo 'olist_full_dataset.csv' foi criado com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea2c9da",
   "metadata": {},
   "source": [
    "### Criando dataset monthly_revenue.csv para prediﾃｧﾃ｣o de faturamento mensal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03a41c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparando dados para a sﾃｩrie temporal de faturamento...\n",
      "Criando a sﾃｩrie temporal de faturamento mensal...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pacie\\AppData\\Local\\Temp\\ipykernel_28908\\1667327117.py:40: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  pd.Grouper(key='order_purchase_datetime', freq='M')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criando features de lag e temporais...\n",
      "\n",
      "DataFrame de Faturamento Mensal com Features de Lag:\n",
      "        data                                 faturamento_mensal  \\\n",
      "3 2016-12-31                                              19.62   \n",
      "4 2017-01-31  19.6219.6211.6216.6218.6219.6220.6219.6219.621...   \n",
      "5 2017-02-28  174.5452.7847.6870.28176.3970.2840.52179.3559....   \n",
      "6 2017-03-31  52.1849.96118.0343.05188.1849.52147.0666.9940....   \n",
      "7 2017-04-30  84.1536.86204.08161.05103.1143.05114.79113.768...   \n",
      "\n",
      "                            faturamento_mes_anterior  \\\n",
      "3                                                  0   \n",
      "4                                              19.62   \n",
      "5  19.6219.6211.6216.6218.6219.6220.6219.6219.621...   \n",
      "6  174.5452.7847.6870.28176.3970.2840.52179.3559....   \n",
      "7  52.1849.96118.0343.05188.1849.52147.0666.9940....   \n",
      "\n",
      "                           faturamento_2_meses_atras  \\\n",
      "3  109.3445.4639.0935.6153.73133.4640.95154.5792....   \n",
      "4                                                  0   \n",
      "5                                              19.62   \n",
      "6  19.6219.6211.6216.6218.6219.6220.6219.6219.621...   \n",
      "7  174.5452.7847.6870.28176.3970.2840.52179.3559....   \n",
      "\n",
      "                           faturamento_3_meses_atras  mes   ano  \n",
      "3                                   136.2375.0640.95   12  2016  \n",
      "4  109.3445.4639.0935.6153.73133.4640.95154.5792....    1  2017  \n",
      "5                                                  0    2  2017  \n",
      "6                                              19.62    3  2017  \n",
      "7  19.6219.6211.6216.6218.6219.6220.6219.6219.621...    4  2017  \n",
      "\n",
      "DataFrame final tem o formato: (23, 7)\n",
      "Salvando o DataFrame de faturamento mensal em 'olist_monthly_revenue.csv'...\n",
      "Arquivo 'olist_full_dataset.csv' criado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Supondo que df_orders e df_order_payments jﾃ｡ estﾃ｣o carregados\n",
    "\n",
    "# 庁 FUNﾃﾃグ DE CONVERSﾃグ CORRIGIDA\n",
    "# Esta funﾃｧﾃ｣o garante que timestamps muito grandes sejam convertidos corretamente\n",
    "def safe_timestamp_to_datetime(series):\n",
    "    series = pd.to_numeric(series, errors='coerce')\n",
    "    if series.dropna().empty:\n",
    "        return pd.NaT\n",
    "    max_val = series.max()\n",
    "    if max_val > 1e18:           # nanosegundos\n",
    "        unit = 'ns'\n",
    "    elif max_val > 1e12:         # milissegundos\n",
    "        unit = 'ms'\n",
    "    else:                        # segundos\n",
    "        unit = 's'\n",
    "    return pd.to_datetime(series, unit=unit, errors='coerce')\n",
    "\n",
    "\n",
    "# --- 1. Junﾃｧﾃ｣o e Preparaﾃｧﾃ｣o dos Dados ---\n",
    "print(\"Preparando dados para a sﾃｩrie temporal de faturamento...\")\n",
    "\n",
    "# Junta pedidos e pagamentos\n",
    "df_revenue = df_orders.merge(\n",
    "    df_order_payments[['order_id', 'payment_value']],\n",
    "    on='order_id',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# 庁 CORREﾃﾃグ: Usa a funﾃｧﾃ｣o segura para converter o timestamp\n",
    "df_revenue['order_purchase_datetime'] = safe_timestamp_to_datetime(df_revenue['order_purchase_timestamp'])\n",
    "\n",
    "\n",
    "# --- 2. Criaﾃｧﾃ｣o da Sﾃｩrie Temporal de Faturamento Mensal ---\n",
    "print(\"Criando a sﾃｩrie temporal de faturamento mensal...\")\n",
    "\n",
    "# Agrupa por mﾃｪs e soma o valor dos pagamentos\n",
    "monthly_revenue = df_revenue.groupby(\n",
    "    pd.Grouper(key='order_purchase_datetime', freq='M')\n",
    ")['payment_value'].sum().reset_index()\n",
    "\n",
    "# Renomeia as colunas\n",
    "monthly_revenue.rename(columns={'order_purchase_datetime': 'data', 'payment_value': 'faturamento_mensal'}, inplace=True)\n",
    "\n",
    "\n",
    "# --- 3. Criaﾃｧﾃ｣o das Features de Lag e Temporais ---\n",
    "print(\"Criando features de lag e temporais...\")\n",
    "\n",
    "# Features de Lag (faturamento dos meses anteriores)\n",
    "monthly_revenue['faturamento_mes_anterior'] = monthly_revenue['faturamento_mensal'].shift(1)\n",
    "monthly_revenue['faturamento_2_meses_atras'] = monthly_revenue['faturamento_mensal'].shift(2)\n",
    "monthly_revenue['faturamento_3_meses_atras'] = monthly_revenue['faturamento_mensal'].shift(3)\n",
    "\n",
    "# Features Temporais\n",
    "monthly_revenue['mes'] = monthly_revenue['data'].dt.month\n",
    "monthly_revenue['ano'] = monthly_revenue['data'].dt.year\n",
    "\n",
    "# Remove as linhas com valores nulos (os primeiros meses que nﾃ｣o tﾃｪm lag)\n",
    "monthly_revenue.dropna(inplace=True)\n",
    "\n",
    "# Exibe o DataFrame final\n",
    "print(\"\\nDataFrame de Faturamento Mensal com Features de Lag:\")\n",
    "print(monthly_revenue.head())\n",
    "print(f\"\\nDataFrame final tem o formato: {monthly_revenue.shape}\")\n",
    "# --- Adicione este trecho de cﾃｳdigo ao final da sua cﾃｩlula ---\n",
    "\n",
    "# Salva o DataFrame final em um arquivo CSV\n",
    "print(\"Salvando o DataFrame de faturamento mensal em 'olist_monthly_revenue.csv'...\")\n",
    "monthly_revenue.to_csv('olist_monthly_revenue.csv', index=False)\n",
    "\n",
    "print(\"Arquivo 'olist_full_dataset.csv' criado com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ab4eeb",
   "metadata": {},
   "source": [
    "### Anﾃ｡lise e Treinamento do modelo para prever o faturamento de um mﾃｪs X\n",
    "- Prediﾃｧﾃ｣o falha/insuficiente com base nos dados de teste.\n",
    "- Modelo nﾃ｣on utilizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db284e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recriando e limpando o DataFrame de faturamento mensal...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pacie\\AppData\\Local\\Temp\\ipykernel_28908\\2723260769.py:38: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  pd.Grouper(key='order_purchase_datetime', freq='M')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Previsﾃ｣o de Faturamento do ﾃ嗟timo Mﾃｪs ---\n",
      "Dados de treino: 22 meses\n",
      "Dados de teste: 1 mﾃｪs\n",
      "\n",
      "--- Anﾃ｡lise da Prediﾃｧﾃ｣o ---\n",
      "Faturamento real do ﾃｺltimo mﾃｪs: R$ 589.67\n",
      "Faturamento previsto para o ﾃｺltimo mﾃｪs: R$ 549333.32\n",
      "Diferenﾃｧa (Real - Previsto): R$ -548743.65\n",
      "Erro percentual: 93059.45%\n",
      "\n",
      "--- Salvando o DataFrame de Faturamento Mensal ---\n",
      "Arquivo 'olist_monthly_revenue.csv' criado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Supondo que df_orders e df_order_payments jﾃ｡ estﾃ｣o carregados na sua sessﾃ｣o\n",
    "\n",
    "# 庁 FUNﾃﾃグ DE CONVERSﾃグ: Necessﾃ｡ria para lidar com os timestamps\n",
    "def safe_timestamp_to_datetime(series):\n",
    "    series = pd.to_numeric(series, errors='coerce')\n",
    "    if series.dropna().empty:\n",
    "        return pd.NaT\n",
    "    max_val = series.max()\n",
    "    if max_val > 1e18:           # nanosegundos\n",
    "        unit = 'ns'\n",
    "    elif max_val > 1e12:         # milissegundos\n",
    "        unit = 'ms'\n",
    "    else:                        # segundos\n",
    "        unit = 's'\n",
    "    return pd.to_datetime(series, unit=unit, errors='coerce')\n",
    "\n",
    "\n",
    "# --- 1. Criaﾃｧﾃ｣o e Limpeza da Sﾃｩrie Temporal de Faturamento ---\n",
    "print(\"Recriando e limpando o DataFrame de faturamento mensal...\")\n",
    "\n",
    "df_revenue = df_orders.merge(\n",
    "    df_order_payments[['order_id', 'payment_value']],\n",
    "    on='order_id',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "df_revenue['payment_value'] = pd.to_numeric(df_revenue['payment_value'], errors='coerce')\n",
    "df_revenue['order_purchase_datetime'] = safe_timestamp_to_datetime(df_revenue['order_purchase_timestamp'])\n",
    "\n",
    "monthly_revenue = df_revenue.groupby(\n",
    "    pd.Grouper(key='order_purchase_datetime', freq='M')\n",
    ")['payment_value'].sum().reset_index()\n",
    "\n",
    "monthly_revenue.rename(columns={'order_purchase_datetime': 'data', 'payment_value': 'faturamento_mensal'}, inplace=True)\n",
    "\n",
    "# Criaﾃｧﾃ｣o das Features de Lag e Temporais\n",
    "monthly_revenue['faturamento_mes_anterior'] = monthly_revenue['faturamento_mensal'].shift(1)\n",
    "monthly_revenue['faturamento_2_meses_atras'] = monthly_revenue['faturamento_mensal'].shift(2)\n",
    "monthly_revenue['faturamento_3_meses_atras'] = monthly_revenue['faturamento_mensal'].shift(3)\n",
    "\n",
    "monthly_revenue['mes'] = monthly_revenue['data'].dt.month\n",
    "monthly_revenue['ano'] = monthly_revenue['data'].dt.year\n",
    "\n",
    "monthly_revenue.dropna(inplace=True)\n",
    "\n",
    "\n",
    "# --- 2. Treinamento e Prediﾃｧﾃ｣o com Random Forest Regressor ---\n",
    "print(\"\\n--- Previsﾃ｣o de Faturamento do ﾃ嗟timo Mﾃｪs ---\")\n",
    "\n",
    "# 庁 CORREﾃﾃグ: Usando o Random Forest Regressor como modelo principal\n",
    "features = [\n",
    "    'faturamento_mes_anterior',\n",
    "    'faturamento_2_meses_atras',\n",
    "    'faturamento_3_meses_atras',\n",
    "    'mes',\n",
    "    'ano'\n",
    "]\n",
    "\n",
    "X = monthly_revenue[features]\n",
    "y = monthly_revenue['faturamento_mensal']\n",
    "\n",
    "X_train = X.iloc[:-1]\n",
    "y_train = y.iloc[:-1]\n",
    "X_test = X.iloc[-1:]\n",
    "y_test = y.iloc[-1:]\n",
    "\n",
    "print(f\"Dados de treino: {len(X_train)} meses\")\n",
    "print(f\"Dados de teste: {len(X_test)} mﾃｪs\")\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# --- 3. Anﾃ｡lise e Comparaﾃｧﾃ｣o ---\n",
    "print(\"\\n--- Anﾃ｡lise da Prediﾃｧﾃ｣o ---\")\n",
    "print(f\"Faturamento real do ﾃｺltimo mﾃｪs: R$ {y_test.values[0]:.2f}\")\n",
    "print(f\"Faturamento previsto para o ﾃｺltimo mﾃｪs: R$ {y_pred[0]:.2f}\")\n",
    "\n",
    "diferenca = y_test.values[0] - y_pred[0]\n",
    "print(f\"Diferenﾃｧa (Real - Previsto): R$ {diferenca:.2f}\")\n",
    "\n",
    "erro_percentual = (abs(diferenca) / y_test.values[0]) * 100\n",
    "print(f\"Erro percentual: {erro_percentual:.2f}%\")\n",
    "\n",
    "\n",
    "# --- 4. Salvando o CSV ---\n",
    "print(\"\\n--- Salvando o DataFrame de Faturamento Mensal ---\")\n",
    "monthly_revenue.to_csv('olist_monthly_revenue.csv', index=False)\n",
    "print(\"Arquivo 'olist_monthly_revenue.csv' criado com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b578d4ca",
   "metadata": {},
   "source": [
    "- Os dados sﾃ｣o insuficientes para gerar um treinamento que possa predizer com um grau elevado de confianﾃｧa o faturamento do mﾃｪs alvo.\n",
    "- Confira o grﾃ｡fico gerado no arquivo EDA onde mostra o aumento e diminuiﾃｧﾃ｣o das vendas ao decorrer dos meses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a004df",
   "metadata": {},
   "source": [
    "### Treinamento e salvando modelo preditor de satisfaﾃｧﾃ｣o do cliente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f336aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando a preparaﾃｧﾃ｣o dos dados...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pacie\\AppData\\Local\\Temp\\ipykernel_20604\\1712715031.py:38: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_full_orders[col].fillna(median_val, inplace=True)\n",
      "C:\\Users\\pacie\\AppData\\Local\\Temp\\ipykernel_20604\\1712715031.py:38: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_full_orders[col].fillna(median_val, inplace=True)\n",
      "C:\\Users\\pacie\\AppData\\Local\\Temp\\ipykernel_20604\\1712715031.py:38: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_full_orders[col].fillna(median_val, inplace=True)\n",
      "C:\\Users\\pacie\\AppData\\Local\\Temp\\ipykernel_20604\\1712715031.py:38: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_full_orders[col].fillna(median_val, inplace=True)\n",
      "C:\\Users\\pacie\\AppData\\Local\\Temp\\ipykernel_20604\\1712715031.py:38: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_full_orders[col].fillna(median_val, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Modelagem para prever se o cliente estﾃ｡ satisfeito ---\n",
      "\n",
      "Treinando Logistic Regression...\n",
      "Mﾃｩtricas para Logistic Regression:\n",
      "Acurﾃ｡cia: 0.7699\n",
      "Precisﾃ｣o: 0.7700\n",
      "Recall: 0.9999\n",
      "F1-Score: 0.8700\n",
      "\n",
      "Treinando Random Forest Classifier...\n",
      "Mﾃｩtricas para Random Forest Classifier:\n",
      "Acurﾃ｡cia: 0.9647\n",
      "Precisﾃ｣o: 0.9710\n",
      "Recall: 0.9836\n",
      "F1-Score: 0.9772\n",
      "\n",
      "Treinando XGBoost Classifier...\n",
      "Mﾃｩtricas para XGBoost Classifier:\n",
      "Acurﾃ｡cia: 0.7743\n",
      "Precisﾃ｣o: 0.7740\n",
      "Recall: 0.9983\n",
      "F1-Score: 0.8720\n",
      "\n",
      "--- Salvando o modelo e o encoder ---\n",
      "Modelo e encoder salvos com sucesso!\n",
      "\n",
      "--- Previsﾃ｣o de Delivery Delay Hours ---\n",
      "\n",
      "Treinando Linear Regression...\n",
      "Mﾃｩtricas para Linear Regression:\n",
      "Rﾂｲ: 0.0162\n",
      "MAE: 161.3275\n",
      "RMSE: 243.2991\n",
      "\n",
      "Treinando Random Forest Regressor...\n",
      "Mﾃｩtricas para Random Forest Regressor:\n",
      "Rﾂｲ: 0.9003\n",
      "MAE: 32.6270\n",
      "RMSE: 77.4628\n",
      "\n",
      "Treinando XGBoost Regressor...\n",
      "Mﾃｩtricas para XGBoost Regressor:\n",
      "Rﾂｲ: 0.0633\n",
      "MAE: 156.9486\n",
      "RMSE: 237.4057\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# --- 1. Preparaﾃｧﾃ｣o dos Dados ---\n",
    "print(\"Iniciando a preparaﾃｧﾃ｣o dos dados...\")\n",
    "\n",
    "# Lista completa de features numﾃｩricas e categﾃｳricas\n",
    "all_numeric_features = [\n",
    "    'price',\n",
    "    'freight_value',\n",
    "    'payment_installments',\n",
    "    #'total_delivery_time_hours',\n",
    "    #'shipping_time_hours',\n",
    "    'product_weight_g',\n",
    "    'product_volume_cm3'\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    'customer_state'\n",
    "]\n",
    "\n",
    "# 庁 CORREﾃﾃグ CRUCIAL: Converte todas as features numﾃｩricas para o tipo correto\n",
    "for col in all_numeric_features:\n",
    "    df_full_orders[col] = pd.to_numeric(df_full_orders[col], errors='coerce')\n",
    "\n",
    "# Preenche os valores nulos com a mediana para as colunas numﾃｩricas\n",
    "for col in all_numeric_features:\n",
    "    median_val = df_full_orders[col].median()\n",
    "    df_full_orders[col].fillna(median_val, inplace=True)\n",
    "\n",
    "# 庁 CORREﾃﾃグ: Garante que a coluna review_score seja numﾃｩrica e sem nulos ANTES de usﾃ｡-la\n",
    "df_full_orders['review_score'] = pd.to_numeric(df_full_orders['review_score'], errors='coerce')\n",
    "df_full_orders.dropna(subset=['review_score', 'delivery_delay_hours'], inplace=True)\n",
    "\n",
    "# 庁 NOVO: Codifica as features categﾃｳricas com One-Hot Encoding\n",
    "one_hot_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "encoded_features = one_hot_encoder.fit_transform(df_full_orders[categorical_features])\n",
    "encoded_df = pd.DataFrame(encoded_features, columns=one_hot_encoder.get_feature_names_out(categorical_features))\n",
    "encoded_df.index = df_full_orders.index\n",
    "\n",
    "# Combina as features numﾃｩricas e categﾃｳricas\n",
    "X_combined = pd.concat([df_full_orders[all_numeric_features], encoded_df], axis=1)\n",
    "\n",
    "\n",
    "# --- 2. Modelagem para Prever 'is_satisfied' (Classificaﾃｧﾃ｣o) ---\n",
    "print(\"\\n--- Modelagem para prever se o cliente estﾃ｡ satisfeito ---\")\n",
    "\n",
    "# 庁 CORREﾃﾃグ: Garante que a coluna review_score seja numﾃｩrica e sem nulos ANTES de usﾃ｡-la\n",
    "df_full_orders['is_satisfied'] = df_full_orders['review_score'].apply(lambda score: 1 if score >= 4 else 0)\n",
    "X = X_combined\n",
    "y_satisfied = df_full_orders['is_satisfied']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_satisfied, test_size=0.20, random_state=42\n",
    ")\n",
    "\n",
    "models_classification = {\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=42, solver='liblinear'),\n",
    "    \"Random Forest Classifier\": RandomForestClassifier(n_estimators=10, random_state=42),\n",
    "    \"XGBoost Classifier\": XGBClassifier(n_estimators=10, random_state=42)\n",
    "}\n",
    "\n",
    "for name, model in models_classification.items():\n",
    "    print(f\"\\nTreinando {name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    print(f\"Mﾃｩtricas para {name}:\")\n",
    "    print(f\"Acurﾃ｡cia: {accuracy:.4f}\")\n",
    "    print(f\"Precisﾃ｣o: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "# --- 庁 NOVO: Salvando o modelo e o encoder ---\n",
    "print(\"\\n--- Salvando o modelo e o encoder ---\")\n",
    "# Define o diretﾃｳrio para salvar\n",
    "model_dir = 'models'\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "# Salva o modelo de classificaﾃｧﾃ｣o que vocﾃｪ quer (ex: Random Forest)\n",
    "joblib.dump(models_classification['Random Forest Classifier'], os.path.join(model_dir, 'rf_classifier_satisfied.joblib'))\n",
    "# Salva o encoder para garantir que a API use a mesma codificaﾃｧﾃ｣o\n",
    "joblib.dump(one_hot_encoder, os.path.join(model_dir, 'one_hot_encoder.joblib'))\n",
    "print(\"Modelo e encoder salvos com sucesso!\")\n",
    "\n",
    "\n",
    "# --- 3. Modelagem para Prever 'delivery_delay_hours' (Regressﾃ｣o) ---\n",
    "print(\"\\n--- Previsﾃ｣o de Delivery Delay Hours ---\")\n",
    "\n",
    "# Garantindo que a coluna de atraso estﾃ｡ pronta para o modelo\n",
    "df_full_orders['delivery_delay_hours'] = pd.to_numeric(df_full_orders['delivery_delay_hours'], errors='coerce')\n",
    "df_full_orders.dropna(subset=['delivery_delay_hours'], inplace=True)\n",
    "\n",
    "X_delay = X_combined\n",
    "y_delay = df_full_orders['delivery_delay_hours']\n",
    "\n",
    "X_train_delay, X_test_delay, y_train_delay, y_test_delay = train_test_split(\n",
    "    X_delay, y_delay, test_size=0.20, random_state=42\n",
    ")\n",
    "\n",
    "models_regression = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Random Forest Regressor\": RandomForestRegressor(n_estimators=10, random_state=42),\n",
    "    \"XGBoost Regressor\": XGBRegressor(n_estimators=10, random_state=42)\n",
    "}\n",
    "\n",
    "for name, model in models_regression.items():\n",
    "    print(f\"\\nTreinando {name}...\")\n",
    "    model.fit(X_train_delay, y_train_delay)\n",
    "    y_pred_delay = model.predict(X_test_delay)\n",
    "    r2 = r2_score(y_test_delay, y_pred_delay)\n",
    "    mae = mean_absolute_error(y_test_delay, y_pred_delay)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_delay, y_pred_delay))\n",
    "    print(f\"Mﾃｩtricas para {name}:\")\n",
    "    print(f\"Rﾂｲ: {r2:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960a916a",
   "metadata": {},
   "source": [
    "### Modelo para prediﾃｧﾃ｣o de frete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eecbc329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparando dados para a prediﾃｧﾃ｣o do valor do frete...\n",
      "\n",
      "Definindo features e limpando dados...\n",
      "\n",
      "--- Treinando e avaliando os modelos de regressﾃ｣o ---\n",
      "\n",
      "Treinando Random Forest Regressor...\n",
      "Mﾃｩtricas para Random Forest Regressor:\n",
      "Rﾂｲ: 0.9609\n",
      "MAE: R$ 1.17\n",
      "RMSE: R$ 3.22\n",
      "\n",
      "Treinando XGBoost Regressor...\n",
      "Mﾃｩtricas para XGBoost Regressor:\n",
      "Rﾂｲ: 0.8231\n",
      "MAE: R$ 3.73\n",
      "RMSE: R$ 6.84\n",
      "\n",
      "--- Salvando o modelo e os prﾃｩ-processadores ---\n",
      "Modelo de frete salvo com sucesso!\n",
      "Scaler e encoder salvos com sucesso!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Supondo que as tabelas df_orders, df_order_items, df_products, df_sellers,\n",
    "# df_customers, df_product_cat_trans jﾃ｡ estﾃ｣o carregadas na sua sessﾃ｣o.\n",
    "\n",
    "# FUNﾃﾃグ DE CONVERSﾃグ: Necessﾃ｡ria para lidar com os timestamps\n",
    "def safe_timestamp_to_datetime(series):\n",
    "    series = pd.to_numeric(series, errors='coerce')\n",
    "    if series.dropna().empty:\n",
    "        return pd.NaT\n",
    "    max_val = series.max()\n",
    "    if max_val > 1e18:           # nanosegundos\n",
    "        unit = 'ns'\n",
    "    elif max_val > 1e12:         # milissegundos\n",
    "        unit = 'ms'\n",
    "    else:                        # segundos\n",
    "        unit = 's'\n",
    "    return pd.to_datetime(series, unit=unit, errors='coerce')\n",
    "\n",
    "\n",
    "# --- 1. Preparaﾃｧﾃ｣o dos Dados para a Modelagem ---\n",
    "print(\"Preparando dados para a prediﾃｧﾃ｣o do valor do frete...\")\n",
    "\n",
    "# Converte colunas para o tipo numﾃｩrico\n",
    "df_order_items['freight_value'] = pd.to_numeric(df_order_items['freight_value'], errors='coerce')\n",
    "df_order_items['price'] = pd.to_numeric(df_order_items['price'], errors='coerce')\n",
    "df_products['product_weight_g'] = pd.to_numeric(df_products['product_weight_g'], errors='coerce')\n",
    "df_products['product_volume_cm3'] = pd.to_numeric(df_products['product_volume_cm3'], errors='coerce')\n",
    "df_products['product_length_cm'] = pd.to_numeric(df_products['product_length_cm'], errors='coerce')\n",
    "df_products['product_height_cm'] = pd.to_numeric(df_products['product_height_cm'], errors='coerce')\n",
    "df_products['product_width_cm'] = pd.to_numeric(df_products['product_width_cm'], errors='coerce')\n",
    "\n",
    "\n",
    "# Junta as tabelas para criar um DataFrame completo de pedidos\n",
    "df_freight_data = df_order_items.merge(\n",
    "    df_orders[['order_id', 'customer_id']], on='order_id', how='inner'\n",
    ").merge(\n",
    "    df_products[['product_id', 'product_weight_g', 'product_volume_cm3', 'product_length_cm', 'product_height_cm', 'product_width_cm']],\n",
    "    on='product_id', how='left'\n",
    ").merge(\n",
    "    df_customers[['customer_id', 'customer_state']], on='customer_id', how='left'\n",
    ").merge(\n",
    "    df_sellers[['seller_id', 'seller_state']], on='seller_id', how='left'\n",
    ")\n",
    "\n",
    "# --- 2. Definiﾃｧﾃ｣o das Features e Limpeza de Dados ---\n",
    "print(\"\\nDefinindo features e limpando dados...\")\n",
    "\n",
    "# Variﾃ｡vel alvo: valor do frete\n",
    "y = df_freight_data['freight_value']\n",
    "\n",
    "# Features para a prediﾃｧﾃ｣o\n",
    "numeric_features = [\n",
    "    'price',\n",
    "    'product_weight_g',\n",
    "    'product_volume_cm3',\n",
    "    'product_length_cm',\n",
    "    'product_height_cm',\n",
    "    'product_width_cm'\n",
    "]\n",
    "\n",
    "categorical_features = ['customer_state', 'seller_state']\n",
    "\n",
    "# Remove linhas com valores nulos nas features e no alvo\n",
    "all_features_and_target = numeric_features + categorical_features + ['freight_value']\n",
    "df_freight_data.dropna(subset=all_features_and_target, inplace=True)\n",
    "df_freight_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "# --- 3. Treinamento e Avaliaﾃｧﾃ｣o do Modelo ---\n",
    "print(\"\\n--- Treinando e avaliando os modelos de regressﾃ｣o ---\")\n",
    "\n",
    "# Codifica as features categﾃｳricas\n",
    "one_hot_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "encoded_features = one_hot_encoder.fit_transform(df_freight_data[categorical_features])\n",
    "encoded_df = pd.DataFrame(encoded_features, columns=one_hot_encoder.get_feature_names_out(categorical_features))\n",
    "encoded_df.index = df_freight_data.index\n",
    "\n",
    "# Combina as features numﾃｩricas e codificadas\n",
    "X = pd.concat([df_freight_data[numeric_features], encoded_df], axis=1)\n",
    "\n",
    "# Padroniza as features numﾃｩricas\n",
    "scaler = StandardScaler()\n",
    "X_scaled = X.copy()\n",
    "X_scaled[numeric_features] = scaler.fit_transform(X_scaled[numeric_features])\n",
    "\n",
    "y_final = df_freight_data['freight_value']\n",
    "\n",
    "\n",
    "# Divisﾃ｣o de treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_final, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "models = {\n",
    "    \"Random Forest Regressor\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    \"XGBoost Regressor\": XGBRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "rf_model_freight = None\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTreinando {name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "    print(f\"Mﾃｩtricas para {name}:\")\n",
    "    print(f\"Rﾂｲ: {r2:.4f}\")\n",
    "    print(f\"MAE: R$ {mae:.2f}\")\n",
    "    print(f\"RMSE: R$ {rmse:.2f}\")\n",
    "    if name == \"Random Forest Regressor\":\n",
    "        rf_model_freight = model\n",
    "\n",
    "\n",
    "# --- 庁 NOVO: Salvando o modelo e os prﾃｩ-processadores ---\n",
    "print(\"\\n--- Salvando o modelo e os prﾃｩ-processadores ---\")\n",
    "\n",
    "model_dir = os.path.join('..', 'models', 'freight_predict')\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "if rf_model_freight is not None:\n",
    "    joblib.dump(rf_model_freight, os.path.join(model_dir, 'rf_regressor_freight.joblib'))\n",
    "    print(\"Modelo de frete salvo com sucesso!\")\n",
    "\n",
    "joblib.dump(scaler, os.path.join(model_dir, 'freight_scaler.joblib'))\n",
    "joblib.dump(one_hot_encoder, os.path.join(model_dir, 'freight_encoder.joblib'))\n",
    "print(\"Scaler e encoder salvos com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96bc1b6",
   "metadata": {},
   "source": [
    "### Obtendo um valor real para testar o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422653c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando modelo e dados para teste...\n",
      "Erro: Arquivo nﾃ｣o encontrado. Verifique se a cﾃｩlula de treino foi executada: [Errno 2] No such file or directory: '..\\\\models\\\\freight_predict\\\\freight_prediction_dataset.csv'\n",
      "\n",
      "Preparando dados para a previsﾃ｣o...\n",
      "\n",
      "--- Extraindo e preparando um JSON para o teste da API ---\n",
      "Aqui estﾃ｣o os dados reais de uma amostra do seu conjunto de teste:\n",
      "Valor Real do Frete: R$ 12.65\n",
      "\n",
      "JSON de Exemplo para a Requisiﾃｧﾃ｣o POST:\n",
      "{\n",
      "    \"price\": 74.9,\n",
      "    \"product_weight_g\": 107.0,\n",
      "    \"product_volume_cm3\": 3211.0,\n",
      "    \"product_length_cm\": 19.0,\n",
      "    \"product_height_cm\": 13.0,\n",
      "    \"product_width_cm\": 13.0,\n",
      "    \"customer_state\": \"SP\",\n",
      "    \"seller_state\": \"PR\"\n",
      "}\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import json\n",
    "# --- 1. Carregando os Artefatos Salvos ---\n",
    "print(\"Carregando modelo e dados para teste...\")\n",
    "\n",
    "model_dir = os.path.join('..', 'models', 'freight_predict')\n",
    "scaler_path = os.path.join(model_dir, 'freight_scaler.joblib')\n",
    "encoder_path = os.path.join(model_dir, 'freight_encoder.joblib')\n",
    "data_path = os.path.join(model_dir, 'freight_prediction_dataset.csv')\n",
    "\n",
    "try:\n",
    "    loaded_scaler = joblib.load(scaler_path)\n",
    "    loaded_encoder = joblib.load(encoder_path)\n",
    "    df_freight_data = pd.read_csv(data_path)\n",
    "    print(\"Arquivos carregados com sucesso!\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Erro: Arquivo nﾃ｣o encontrado. Verifique se a cﾃｩlula de treino foi executada: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. Preparando os Dados de Teste ---\n",
    "print(\"\\nPreparando dados para a previsﾃ｣o...\")\n",
    "\n",
    "numeric_features = [\n",
    "    'price', 'product_weight_g', 'product_volume_cm3', 'product_length_cm',\n",
    "    'product_height_cm', 'product_width_cm'\n",
    "]\n",
    "categorical_features = ['customer_state', 'seller_state']\n",
    "\n",
    "# Remove linhas com valores nulos nas features e no alvo\n",
    "all_features_and_target = numeric_features + categorical_features + ['freight_value']\n",
    "df_freight_data.dropna(subset=all_features_and_target, inplace=True)\n",
    "df_freight_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Cria as features e o alvo (em seus valores originais)\n",
    "X = df_freight_data[numeric_features + categorical_features]\n",
    "y = df_freight_data['freight_value']\n",
    "\n",
    "# Recria o MESMO conjunto de treino/teste usado no treino\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# --- 3. Extraindo e Revertendo uma Amostra Real ---\n",
    "print(\"\\n--- Extraindo e preparando um JSON para o teste da API ---\")\n",
    "\n",
    "# Escolhe a primeira amostra do conjunto de teste para a previsﾃ｣o\n",
    "sample_index = 0\n",
    "sample_data = X_test.iloc[sample_index].to_dict()\n",
    "sample_real_freight = y_test.iloc[sample_index]\n",
    "\n",
    "print(\"Aqui estﾃ｣o os dados reais de uma amostra do seu conjunto de teste:\")\n",
    "print(f\"Valor Real do Frete: R$ {sample_real_freight:.2f}\")\n",
    "\n",
    "# Exibe o JSON que vocﾃｪ deve usar no seu arquivo de teste\n",
    "print(\"\\nJSON de Exemplo para a Requisiﾃｧﾃ｣o POST:\")\n",
    "json_data = json.dumps(sample_data, indent=4)\n",
    "print(json_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
