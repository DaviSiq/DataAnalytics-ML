{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1daea395",
   "metadata": {},
   "source": [
    "### Modelagem preditiva\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cd45b1",
   "metadata": {},
   "source": [
    "### Salvando de volta para csv (com todas as tabelas) para usar exemplos reais"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5b6afc",
   "metadata": {},
   "source": [
    "#### Juntando todas as tabelas em uma s√≥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c858c06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando tabelas do Snowflake...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pacie\\AppData\\Local\\Temp\\ipykernel_17392\\1674283844.py:29: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_orders = pd.read_sql(\"SELECT * FROM orders_refined\", conn)\n",
      "C:\\Users\\pacie\\AppData\\Local\\Temp\\ipykernel_17392\\1674283844.py:30: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_orders_reviews = pd.read_sql(\"SELECT * FROM order_reviews_refined\", conn)\n",
      "C:\\Users\\pacie\\AppData\\Local\\Temp\\ipykernel_17392\\1674283844.py:31: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_order_payments = pd.read_sql(\"SELECT * FROM order_payments_refined\", conn)\n",
      "C:\\Users\\pacie\\AppData\\Local\\Temp\\ipykernel_17392\\1674283844.py:32: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_order_items = pd.read_sql(\"SELECT * FROM order_items_refined\", conn)\n",
      "C:\\Users\\pacie\\AppData\\Local\\Temp\\ipykernel_17392\\1674283844.py:33: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_products = pd.read_sql(\"SELECT * FROM products_refined\", conn)\n",
      "C:\\Users\\pacie\\AppData\\Local\\Temp\\ipykernel_17392\\1674283844.py:34: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_customers = pd.read_sql(\"SELECT * FROM customers_refined\", conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unindo as tabelas...\n",
      "Processo conclu√≠do. O arquivo 'olist_full_dataset.csv' foi criado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import snowflake.connector\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "# Carrega as vari√°veis de ambiente\n",
    "env_path = Path('.') / 'env.env'\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "SF_USER = os.getenv(\"SF_USER\")\n",
    "SF_PASSWORD = os.getenv(\"SF_PASSWORD\")\n",
    "SF_ACCOUNT = os.getenv(\"SF_ACCOUNT\")\n",
    "SF_WAREHOUSE = os.getenv(\"SF_WAREHOUSE\")\n",
    "SF_DATABASE = os.getenv(\"SF_DATABASE\")\n",
    "SF_SCHEMA = os.getenv(\"SF_SCHEMA\")\n",
    "\n",
    "# Conecta ao Snowflake\n",
    "conn = snowflake.connector.connect(\n",
    "    user=SF_USER,\n",
    "    password=SF_PASSWORD,\n",
    "    account=SF_ACCOUNT,\n",
    "    warehouse=SF_WAREHOUSE,\n",
    "    database=SF_DATABASE,\n",
    "    schema=SF_SCHEMA\n",
    ")\n",
    "\n",
    "# Carrega as tabelas\n",
    "print(\"Carregando tabelas do Snowflake...\")\n",
    "df_orders = pd.read_sql(\"SELECT * FROM orders_refined\", conn)\n",
    "df_orders_reviews = pd.read_sql(\"SELECT * FROM order_reviews_refined\", conn)\n",
    "df_order_payments = pd.read_sql(\"SELECT * FROM order_payments_refined\", conn)\n",
    "df_order_items = pd.read_sql(\"SELECT * FROM order_items_refined\", conn)\n",
    "df_products = pd.read_sql(\"SELECT * FROM products_refined\", conn)\n",
    "df_customers = pd.read_sql(\"SELECT * FROM customers_refined\", conn)\n",
    "\n",
    "# Fecha a conex√£o\n",
    "conn.close()\n",
    "\n",
    "# Padroniza os nomes das colunas\n",
    "for df in [df_orders, df_orders_reviews, df_order_payments, df_order_items, df_products, df_customers]:\n",
    "    df.columns = df.columns.str.lower()\n",
    "\n",
    "# Realiza as jun√ß√µes sequenciais\n",
    "print(\"Unindo as tabelas...\")\n",
    "df_full_orders = df_orders.merge(df_orders_reviews, on='order_id', how='left')\n",
    "df_full_orders = df_full_orders.merge(df_order_payments, on='order_id', how='left')\n",
    "df_full_orders = df_full_orders.merge(df_order_items, on='order_id', how='left')\n",
    "df_full_orders = df_full_orders.merge(df_products, on='product_id', how='left')\n",
    "df_full_orders = df_full_orders.merge(\n",
    "    df_customers[['customer_id', 'customer_state', 'customer_zip_code_prefix']],\n",
    "    on='customer_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Salva o DataFrame completo em um arquivo CSV\n",
    "#print(\"Salvando o DataFrame completo em olist_full_dataset.csv...\")\n",
    "#df_full_orders.to_csv('olist_full_dataset.csv', index=False)\n",
    "\n",
    "print(\"Processo conclu√≠do. O arquivo 'olist_full_dataset.csv' foi criado com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea2c9da",
   "metadata": {},
   "source": [
    "### Criando dataset monthly_revenue.csv para predi√ß√£o de faturamento mensal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03a41c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparando dados para a s√©rie temporal de faturamento...\n",
      "Criando a s√©rie temporal de faturamento mensal...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pacie\\AppData\\Local\\Temp\\ipykernel_28908\\1667327117.py:40: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  pd.Grouper(key='order_purchase_datetime', freq='M')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criando features de lag e temporais...\n",
      "\n",
      "DataFrame de Faturamento Mensal com Features de Lag:\n",
      "        data                                 faturamento_mensal  \\\n",
      "3 2016-12-31                                              19.62   \n",
      "4 2017-01-31  19.6219.6211.6216.6218.6219.6220.6219.6219.621...   \n",
      "5 2017-02-28  174.5452.7847.6870.28176.3970.2840.52179.3559....   \n",
      "6 2017-03-31  52.1849.96118.0343.05188.1849.52147.0666.9940....   \n",
      "7 2017-04-30  84.1536.86204.08161.05103.1143.05114.79113.768...   \n",
      "\n",
      "                            faturamento_mes_anterior  \\\n",
      "3                                                  0   \n",
      "4                                              19.62   \n",
      "5  19.6219.6211.6216.6218.6219.6220.6219.6219.621...   \n",
      "6  174.5452.7847.6870.28176.3970.2840.52179.3559....   \n",
      "7  52.1849.96118.0343.05188.1849.52147.0666.9940....   \n",
      "\n",
      "                           faturamento_2_meses_atras  \\\n",
      "3  109.3445.4639.0935.6153.73133.4640.95154.5792....   \n",
      "4                                                  0   \n",
      "5                                              19.62   \n",
      "6  19.6219.6211.6216.6218.6219.6220.6219.6219.621...   \n",
      "7  174.5452.7847.6870.28176.3970.2840.52179.3559....   \n",
      "\n",
      "                           faturamento_3_meses_atras  mes   ano  \n",
      "3                                   136.2375.0640.95   12  2016  \n",
      "4  109.3445.4639.0935.6153.73133.4640.95154.5792....    1  2017  \n",
      "5                                                  0    2  2017  \n",
      "6                                              19.62    3  2017  \n",
      "7  19.6219.6211.6216.6218.6219.6220.6219.6219.621...    4  2017  \n",
      "\n",
      "DataFrame final tem o formato: (23, 7)\n",
      "Salvando o DataFrame de faturamento mensal em 'olist_monthly_revenue.csv'...\n",
      "Arquivo 'olist_full_dataset.csv' criado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Supondo que df_orders e df_order_payments j√° est√£o carregados\n",
    "\n",
    "# üí° FUN√á√ÉO DE CONVERS√ÉO CORRIGIDA\n",
    "# Esta fun√ß√£o garante que timestamps muito grandes sejam convertidos corretamente\n",
    "def safe_timestamp_to_datetime(series):\n",
    "    series = pd.to_numeric(series, errors='coerce')\n",
    "    if series.dropna().empty:\n",
    "        return pd.NaT\n",
    "    max_val = series.max()\n",
    "    if max_val > 1e18:           # nanosegundos\n",
    "        unit = 'ns'\n",
    "    elif max_val > 1e12:         # milissegundos\n",
    "        unit = 'ms'\n",
    "    else:                        # segundos\n",
    "        unit = 's'\n",
    "    return pd.to_datetime(series, unit=unit, errors='coerce')\n",
    "\n",
    "\n",
    "# --- 1. Jun√ß√£o e Prepara√ß√£o dos Dados ---\n",
    "print(\"Preparando dados para a s√©rie temporal de faturamento...\")\n",
    "\n",
    "# Junta pedidos e pagamentos\n",
    "df_revenue = df_orders.merge(\n",
    "    df_order_payments[['order_id', 'payment_value']],\n",
    "    on='order_id',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# üí° CORRE√á√ÉO: Usa a fun√ß√£o segura para converter o timestamp\n",
    "df_revenue['order_purchase_datetime'] = safe_timestamp_to_datetime(df_revenue['order_purchase_timestamp'])\n",
    "\n",
    "\n",
    "# --- 2. Cria√ß√£o da S√©rie Temporal de Faturamento Mensal ---\n",
    "print(\"Criando a s√©rie temporal de faturamento mensal...\")\n",
    "\n",
    "# Agrupa por m√™s e soma o valor dos pagamentos\n",
    "monthly_revenue = df_revenue.groupby(\n",
    "    pd.Grouper(key='order_purchase_datetime', freq='M')\n",
    ")['payment_value'].sum().reset_index()\n",
    "\n",
    "# Renomeia as colunas\n",
    "monthly_revenue.rename(columns={'order_purchase_datetime': 'data', 'payment_value': 'faturamento_mensal'}, inplace=True)\n",
    "\n",
    "\n",
    "# --- 3. Cria√ß√£o das Features de Lag e Temporais ---\n",
    "print(\"Criando features de lag e temporais...\")\n",
    "\n",
    "# Features de Lag (faturamento dos meses anteriores)\n",
    "monthly_revenue['faturamento_mes_anterior'] = monthly_revenue['faturamento_mensal'].shift(1)\n",
    "monthly_revenue['faturamento_2_meses_atras'] = monthly_revenue['faturamento_mensal'].shift(2)\n",
    "monthly_revenue['faturamento_3_meses_atras'] = monthly_revenue['faturamento_mensal'].shift(3)\n",
    "\n",
    "# Features Temporais\n",
    "monthly_revenue['mes'] = monthly_revenue['data'].dt.month\n",
    "monthly_revenue['ano'] = monthly_revenue['data'].dt.year\n",
    "\n",
    "# Remove as linhas com valores nulos (os primeiros meses que n√£o t√™m lag)\n",
    "monthly_revenue.dropna(inplace=True)\n",
    "\n",
    "# Exibe o DataFrame final\n",
    "print(\"\\nDataFrame de Faturamento Mensal com Features de Lag:\")\n",
    "print(monthly_revenue.head())\n",
    "print(f\"\\nDataFrame final tem o formato: {monthly_revenue.shape}\")\n",
    "# --- Adicione este trecho de c√≥digo ao final da sua c√©lula ---\n",
    "\n",
    "# Salva o DataFrame final em um arquivo CSV\n",
    "print(\"Salvando o DataFrame de faturamento mensal em 'olist_monthly_revenue.csv'...\")\n",
    "monthly_revenue.to_csv('olist_monthly_revenue.csv', index=False)\n",
    "\n",
    "print(\"Arquivo 'olist_full_dataset.csv' criado com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ab4eeb",
   "metadata": {},
   "source": [
    "### An√°lise e Treinamento do modelo para prever o faturamento de um m√™s X\n",
    "- Predi√ß√£o falha/insuficiente com base nos dados de teste.\n",
    "- Modelo n√£on utilizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db284e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recriando e limpando o DataFrame de faturamento mensal...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pacie\\AppData\\Local\\Temp\\ipykernel_28908\\2723260769.py:38: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  pd.Grouper(key='order_purchase_datetime', freq='M')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Previs√£o de Faturamento do √öltimo M√™s ---\n",
      "Dados de treino: 22 meses\n",
      "Dados de teste: 1 m√™s\n",
      "\n",
      "--- An√°lise da Predi√ß√£o ---\n",
      "Faturamento real do √∫ltimo m√™s: R$ 589.67\n",
      "Faturamento previsto para o √∫ltimo m√™s: R$ 549333.32\n",
      "Diferen√ßa (Real - Previsto): R$ -548743.65\n",
      "Erro percentual: 93059.45%\n",
      "\n",
      "--- Salvando o DataFrame de Faturamento Mensal ---\n",
      "Arquivo 'olist_monthly_revenue.csv' criado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Supondo que df_orders e df_order_payments j√° est√£o carregados na sua sess√£o\n",
    "\n",
    "# üí° FUN√á√ÉO DE CONVERS√ÉO: Necess√°ria para lidar com os timestamps\n",
    "def safe_timestamp_to_datetime(series):\n",
    "    series = pd.to_numeric(series, errors='coerce')\n",
    "    if series.dropna().empty:\n",
    "        return pd.NaT\n",
    "    max_val = series.max()\n",
    "    if max_val > 1e18:           # nanosegundos\n",
    "        unit = 'ns'\n",
    "    elif max_val > 1e12:         # milissegundos\n",
    "        unit = 'ms'\n",
    "    else:                        # segundos\n",
    "        unit = 's'\n",
    "    return pd.to_datetime(series, unit=unit, errors='coerce')\n",
    "\n",
    "\n",
    "# --- 1. Cria√ß√£o e Limpeza da S√©rie Temporal de Faturamento ---\n",
    "print(\"Recriando e limpando o DataFrame de faturamento mensal...\")\n",
    "\n",
    "df_revenue = df_orders.merge(\n",
    "    df_order_payments[['order_id', 'payment_value']],\n",
    "    on='order_id',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "df_revenue['payment_value'] = pd.to_numeric(df_revenue['payment_value'], errors='coerce')\n",
    "df_revenue['order_purchase_datetime'] = safe_timestamp_to_datetime(df_revenue['order_purchase_timestamp'])\n",
    "\n",
    "monthly_revenue = df_revenue.groupby(\n",
    "    pd.Grouper(key='order_purchase_datetime', freq='M')\n",
    ")['payment_value'].sum().reset_index()\n",
    "\n",
    "monthly_revenue.rename(columns={'order_purchase_datetime': 'data', 'payment_value': 'faturamento_mensal'}, inplace=True)\n",
    "\n",
    "# Cria√ß√£o das Features de Lag e Temporais\n",
    "monthly_revenue['faturamento_mes_anterior'] = monthly_revenue['faturamento_mensal'].shift(1)\n",
    "monthly_revenue['faturamento_2_meses_atras'] = monthly_revenue['faturamento_mensal'].shift(2)\n",
    "monthly_revenue['faturamento_3_meses_atras'] = monthly_revenue['faturamento_mensal'].shift(3)\n",
    "\n",
    "monthly_revenue['mes'] = monthly_revenue['data'].dt.month\n",
    "monthly_revenue['ano'] = monthly_revenue['data'].dt.year\n",
    "\n",
    "monthly_revenue.dropna(inplace=True)\n",
    "\n",
    "\n",
    "# --- 2. Treinamento e Predi√ß√£o com Random Forest Regressor ---\n",
    "print(\"\\n--- Previs√£o de Faturamento do √öltimo M√™s ---\")\n",
    "\n",
    "# üí° CORRE√á√ÉO: Usando o Random Forest Regressor como modelo principal\n",
    "features = [\n",
    "    'faturamento_mes_anterior',\n",
    "    'faturamento_2_meses_atras',\n",
    "    'faturamento_3_meses_atras',\n",
    "    'mes',\n",
    "    'ano'\n",
    "]\n",
    "\n",
    "X = monthly_revenue[features]\n",
    "y = monthly_revenue['faturamento_mensal']\n",
    "\n",
    "X_train = X.iloc[:-1]\n",
    "y_train = y.iloc[:-1]\n",
    "X_test = X.iloc[-1:]\n",
    "y_test = y.iloc[-1:]\n",
    "\n",
    "print(f\"Dados de treino: {len(X_train)} meses\")\n",
    "print(f\"Dados de teste: {len(X_test)} m√™s\")\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# --- 3. An√°lise e Compara√ß√£o ---\n",
    "print(\"\\n--- An√°lise da Predi√ß√£o ---\")\n",
    "print(f\"Faturamento real do √∫ltimo m√™s: R$ {y_test.values[0]:.2f}\")\n",
    "print(f\"Faturamento previsto para o √∫ltimo m√™s: R$ {y_pred[0]:.2f}\")\n",
    "\n",
    "diferenca = y_test.values[0] - y_pred[0]\n",
    "print(f\"Diferen√ßa (Real - Previsto): R$ {diferenca:.2f}\")\n",
    "\n",
    "erro_percentual = (abs(diferenca) / y_test.values[0]) * 100\n",
    "print(f\"Erro percentual: {erro_percentual:.2f}%\")\n",
    "\n",
    "\n",
    "# --- 4. Salvando o CSV ---\n",
    "print(\"\\n--- Salvando o DataFrame de Faturamento Mensal ---\")\n",
    "monthly_revenue.to_csv('olist_monthly_revenue.csv', index=False)\n",
    "print(\"Arquivo 'olist_monthly_revenue.csv' criado com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b578d4ca",
   "metadata": {},
   "source": [
    "- Os dados s√£o insuficientes para gerar um treinamento que possa predizer com um grau elevado de confian√ßa o faturamento do m√™s alvo.\n",
    "- Confira o gr√°fico gerado no arquivo EDA onde mostra o aumento e diminui√ß√£o das vendas ao decorrer dos meses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a004df",
   "metadata": {},
   "source": [
    "### Treinamento e salvando modelo preditor de satisfa√ß√£o do cliente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f336aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando a prepara√ß√£o dos dados...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pacie\\AppData\\Local\\Temp\\ipykernel_20604\\1712715031.py:38: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_full_orders[col].fillna(median_val, inplace=True)\n",
      "C:\\Users\\pacie\\AppData\\Local\\Temp\\ipykernel_20604\\1712715031.py:38: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_full_orders[col].fillna(median_val, inplace=True)\n",
      "C:\\Users\\pacie\\AppData\\Local\\Temp\\ipykernel_20604\\1712715031.py:38: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_full_orders[col].fillna(median_val, inplace=True)\n",
      "C:\\Users\\pacie\\AppData\\Local\\Temp\\ipykernel_20604\\1712715031.py:38: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_full_orders[col].fillna(median_val, inplace=True)\n",
      "C:\\Users\\pacie\\AppData\\Local\\Temp\\ipykernel_20604\\1712715031.py:38: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_full_orders[col].fillna(median_val, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Modelagem para prever se o cliente est√° satisfeito ---\n",
      "\n",
      "Treinando Logistic Regression...\n",
      "M√©tricas para Logistic Regression:\n",
      "Acur√°cia: 0.7699\n",
      "Precis√£o: 0.7700\n",
      "Recall: 0.9999\n",
      "F1-Score: 0.8700\n",
      "\n",
      "Treinando Random Forest Classifier...\n",
      "M√©tricas para Random Forest Classifier:\n",
      "Acur√°cia: 0.9647\n",
      "Precis√£o: 0.9710\n",
      "Recall: 0.9836\n",
      "F1-Score: 0.9772\n",
      "\n",
      "Treinando XGBoost Classifier...\n",
      "M√©tricas para XGBoost Classifier:\n",
      "Acur√°cia: 0.7743\n",
      "Precis√£o: 0.7740\n",
      "Recall: 0.9983\n",
      "F1-Score: 0.8720\n",
      "\n",
      "--- Salvando o modelo e o encoder ---\n",
      "Modelo e encoder salvos com sucesso!\n",
      "\n",
      "--- Previs√£o de Delivery Delay Hours ---\n",
      "\n",
      "Treinando Linear Regression...\n",
      "M√©tricas para Linear Regression:\n",
      "R¬≤: 0.0162\n",
      "MAE: 161.3275\n",
      "RMSE: 243.2991\n",
      "\n",
      "Treinando Random Forest Regressor...\n",
      "M√©tricas para Random Forest Regressor:\n",
      "R¬≤: 0.9003\n",
      "MAE: 32.6270\n",
      "RMSE: 77.4628\n",
      "\n",
      "Treinando XGBoost Regressor...\n",
      "M√©tricas para XGBoost Regressor:\n",
      "R¬≤: 0.0633\n",
      "MAE: 156.9486\n",
      "RMSE: 237.4057\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# --- 1. Prepara√ß√£o dos Dados ---\n",
    "print(\"Iniciando a prepara√ß√£o dos dados...\")\n",
    "\n",
    "# Lista completa de features num√©ricas e categ√≥ricas\n",
    "all_numeric_features = [\n",
    "    'price',\n",
    "    'freight_value',\n",
    "    'payment_installments',\n",
    "    #'total_delivery_time_hours',\n",
    "    #'shipping_time_hours',\n",
    "    'product_weight_g',\n",
    "    'product_volume_cm3'\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    'customer_state'\n",
    "]\n",
    "\n",
    "# üí° CORRE√á√ÉO CRUCIAL: Converte todas as features num√©ricas para o tipo correto\n",
    "for col in all_numeric_features:\n",
    "    df_full_orders[col] = pd.to_numeric(df_full_orders[col], errors='coerce')\n",
    "\n",
    "# Preenche os valores nulos com a mediana para as colunas num√©ricas\n",
    "for col in all_numeric_features:\n",
    "    median_val = df_full_orders[col].median()\n",
    "    df_full_orders[col].fillna(median_val, inplace=True)\n",
    "\n",
    "# üí° CORRE√á√ÉO: Garante que a coluna review_score seja num√©rica e sem nulos ANTES de us√°-la\n",
    "df_full_orders['review_score'] = pd.to_numeric(df_full_orders['review_score'], errors='coerce')\n",
    "df_full_orders.dropna(subset=['review_score', 'delivery_delay_hours'], inplace=True)\n",
    "\n",
    "# üí° NOVO: Codifica as features categ√≥ricas com One-Hot Encoding\n",
    "one_hot_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "encoded_features = one_hot_encoder.fit_transform(df_full_orders[categorical_features])\n",
    "encoded_df = pd.DataFrame(encoded_features, columns=one_hot_encoder.get_feature_names_out(categorical_features))\n",
    "encoded_df.index = df_full_orders.index\n",
    "\n",
    "# Combina as features num√©ricas e categ√≥ricas\n",
    "X_combined = pd.concat([df_full_orders[all_numeric_features], encoded_df], axis=1)\n",
    "\n",
    "\n",
    "# --- 2. Modelagem para Prever 'is_satisfied' (Classifica√ß√£o) ---\n",
    "print(\"\\n--- Modelagem para prever se o cliente est√° satisfeito ---\")\n",
    "\n",
    "# üí° CORRE√á√ÉO: Garante que a coluna review_score seja num√©rica e sem nulos ANTES de us√°-la\n",
    "df_full_orders['is_satisfied'] = df_full_orders['review_score'].apply(lambda score: 1 if score >= 4 else 0)\n",
    "X = X_combined\n",
    "y_satisfied = df_full_orders['is_satisfied']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_satisfied, test_size=0.20, random_state=42\n",
    ")\n",
    "\n",
    "models_classification = {\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=42, solver='liblinear'),\n",
    "    \"Random Forest Classifier\": RandomForestClassifier(n_estimators=10, random_state=42),\n",
    "    \"XGBoost Classifier\": XGBClassifier(n_estimators=10, random_state=42)\n",
    "}\n",
    "\n",
    "for name, model in models_classification.items():\n",
    "    print(f\"\\nTreinando {name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    print(f\"M√©tricas para {name}:\")\n",
    "    print(f\"Acur√°cia: {accuracy:.4f}\")\n",
    "    print(f\"Precis√£o: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "# --- üí° NOVO: Salvando o modelo e o encoder ---\n",
    "print(\"\\n--- Salvando o modelo e o encoder ---\")\n",
    "# Define o diret√≥rio para salvar\n",
    "model_dir = 'models'\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "# Salva o modelo de classifica√ß√£o que voc√™ quer (ex: Random Forest)\n",
    "joblib.dump(models_classification['Random Forest Classifier'], os.path.join(model_dir, 'rf_classifier_satisfied.joblib'))\n",
    "# Salva o encoder para garantir que a API use a mesma codifica√ß√£o\n",
    "joblib.dump(one_hot_encoder, os.path.join(model_dir, 'one_hot_encoder.joblib'))\n",
    "print(\"Modelo e encoder salvos com sucesso!\")\n",
    "\n",
    "\n",
    "# --- 3. Modelagem para Prever 'delivery_delay_hours' (Regress√£o) ---\n",
    "print(\"\\n--- Previs√£o de Delivery Delay Hours ---\")\n",
    "\n",
    "# Garantindo que a coluna de atraso est√° pronta para o modelo\n",
    "df_full_orders['delivery_delay_hours'] = pd.to_numeric(df_full_orders['delivery_delay_hours'], errors='coerce')\n",
    "df_full_orders.dropna(subset=['delivery_delay_hours'], inplace=True)\n",
    "\n",
    "X_delay = X_combined\n",
    "y_delay = df_full_orders['delivery_delay_hours']\n",
    "\n",
    "X_train_delay, X_test_delay, y_train_delay, y_test_delay = train_test_split(\n",
    "    X_delay, y_delay, test_size=0.20, random_state=42\n",
    ")\n",
    "\n",
    "models_regression = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Random Forest Regressor\": RandomForestRegressor(n_estimators=10, random_state=42),\n",
    "    \"XGBoost Regressor\": XGBRegressor(n_estimators=10, random_state=42)\n",
    "}\n",
    "\n",
    "for name, model in models_regression.items():\n",
    "    print(f\"\\nTreinando {name}...\")\n",
    "    model.fit(X_train_delay, y_train_delay)\n",
    "    y_pred_delay = model.predict(X_test_delay)\n",
    "    r2 = r2_score(y_test_delay, y_pred_delay)\n",
    "    mae = mean_absolute_error(y_test_delay, y_pred_delay)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_delay, y_pred_delay))\n",
    "    print(f\"M√©tricas para {name}:\")\n",
    "    print(f\"R¬≤: {r2:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960a916a",
   "metadata": {},
   "source": [
    "### Modelo para predi√ß√£o de frete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eecbc329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparando dados para a predi√ß√£o do valor do frete...\n",
      "\n",
      "Definindo features e limpando dados...\n",
      "\n",
      "--- Treinando e avaliando os modelos de regress√£o ---\n",
      "\n",
      "Treinando Random Forest Regressor...\n",
      "M√©tricas para Random Forest Regressor:\n",
      "R¬≤: 0.9609\n",
      "MAE: R$ 1.17\n",
      "RMSE: R$ 3.22\n",
      "\n",
      "Treinando XGBoost Regressor...\n",
      "M√©tricas para XGBoost Regressor:\n",
      "R¬≤: 0.8231\n",
      "MAE: R$ 3.73\n",
      "RMSE: R$ 6.84\n",
      "\n",
      "--- Salvando o modelo e os pr√©-processadores ---\n",
      "Modelo de frete salvo com sucesso!\n",
      "Scaler e encoder salvos com sucesso!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Supondo que as tabelas df_orders, df_order_items, df_products, df_sellers,\n",
    "# df_customers, df_product_cat_trans j√° est√£o carregadas na sua sess√£o.\n",
    "\n",
    "# FUN√á√ÉO DE CONVERS√ÉO: Necess√°ria para lidar com os timestamps\n",
    "def safe_timestamp_to_datetime(series):\n",
    "    series = pd.to_numeric(series, errors='coerce')\n",
    "    if series.dropna().empty:\n",
    "        return pd.NaT\n",
    "    max_val = series.max()\n",
    "    if max_val > 1e18:           # nanosegundos\n",
    "        unit = 'ns'\n",
    "    elif max_val > 1e12:         # milissegundos\n",
    "        unit = 'ms'\n",
    "    else:                        # segundos\n",
    "        unit = 's'\n",
    "    return pd.to_datetime(series, unit=unit, errors='coerce')\n",
    "\n",
    "\n",
    "# --- 1. Prepara√ß√£o dos Dados para a Modelagem ---\n",
    "print(\"Preparando dados para a predi√ß√£o do valor do frete...\")\n",
    "\n",
    "# Converte colunas para o tipo num√©rico\n",
    "df_order_items['freight_value'] = pd.to_numeric(df_order_items['freight_value'], errors='coerce')\n",
    "df_order_items['price'] = pd.to_numeric(df_order_items['price'], errors='coerce')\n",
    "df_products['product_weight_g'] = pd.to_numeric(df_products['product_weight_g'], errors='coerce')\n",
    "df_products['product_volume_cm3'] = pd.to_numeric(df_products['product_volume_cm3'], errors='coerce')\n",
    "df_products['product_length_cm'] = pd.to_numeric(df_products['product_length_cm'], errors='coerce')\n",
    "df_products['product_height_cm'] = pd.to_numeric(df_products['product_height_cm'], errors='coerce')\n",
    "df_products['product_width_cm'] = pd.to_numeric(df_products['product_width_cm'], errors='coerce')\n",
    "\n",
    "\n",
    "# Junta as tabelas para criar um DataFrame completo de pedidos\n",
    "df_freight_data = df_order_items.merge(\n",
    "    df_orders[['order_id', 'customer_id']], on='order_id', how='inner'\n",
    ").merge(\n",
    "    df_products[['product_id', 'product_weight_g', 'product_volume_cm3', 'product_length_cm', 'product_height_cm', 'product_width_cm']],\n",
    "    on='product_id', how='left'\n",
    ").merge(\n",
    "    df_customers[['customer_id', 'customer_state']], on='customer_id', how='left'\n",
    ").merge(\n",
    "    df_sellers[['seller_id', 'seller_state']], on='seller_id', how='left'\n",
    ")\n",
    "\n",
    "# --- 2. Defini√ß√£o das Features e Limpeza de Dados ---\n",
    "print(\"\\nDefinindo features e limpando dados...\")\n",
    "\n",
    "# Vari√°vel alvo: valor do frete\n",
    "y = df_freight_data['freight_value']\n",
    "\n",
    "# Features para a predi√ß√£o\n",
    "numeric_features = [\n",
    "    'price',\n",
    "    'product_weight_g',\n",
    "    'product_volume_cm3',\n",
    "    'product_length_cm',\n",
    "    'product_height_cm',\n",
    "    'product_width_cm'\n",
    "]\n",
    "\n",
    "categorical_features = ['customer_state', 'seller_state']\n",
    "\n",
    "# Remove linhas com valores nulos nas features e no alvo\n",
    "all_features_and_target = numeric_features + categorical_features + ['freight_value']\n",
    "df_freight_data.dropna(subset=all_features_and_target, inplace=True)\n",
    "df_freight_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "# --- 3. Treinamento e Avalia√ß√£o do Modelo ---\n",
    "print(\"\\n--- Treinando e avaliando os modelos de regress√£o ---\")\n",
    "\n",
    "# Codifica as features categ√≥ricas\n",
    "one_hot_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "encoded_features = one_hot_encoder.fit_transform(df_freight_data[categorical_features])\n",
    "encoded_df = pd.DataFrame(encoded_features, columns=one_hot_encoder.get_feature_names_out(categorical_features))\n",
    "encoded_df.index = df_freight_data.index\n",
    "\n",
    "# Combina as features num√©ricas e codificadas\n",
    "X = pd.concat([df_freight_data[numeric_features], encoded_df], axis=1)\n",
    "\n",
    "# Padroniza as features num√©ricas\n",
    "scaler = StandardScaler()\n",
    "X_scaled = X.copy()\n",
    "X_scaled[numeric_features] = scaler.fit_transform(X_scaled[numeric_features])\n",
    "\n",
    "y_final = df_freight_data['freight_value']\n",
    "\n",
    "\n",
    "# Divis√£o de treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_final, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "models = {\n",
    "    \"Random Forest Regressor\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    \"XGBoost Regressor\": XGBRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "rf_model_freight = None\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTreinando {name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "    print(f\"M√©tricas para {name}:\")\n",
    "    print(f\"R¬≤: {r2:.4f}\")\n",
    "    print(f\"MAE: R$ {mae:.2f}\")\n",
    "    print(f\"RMSE: R$ {rmse:.2f}\")\n",
    "    if name == \"Random Forest Regressor\":\n",
    "        rf_model_freight = model\n",
    "\n",
    "\n",
    "# --- üí° NOVO: Salvando o modelo e os pr√©-processadores ---\n",
    "print(\"\\n--- Salvando o modelo e os pr√©-processadores ---\")\n",
    "\n",
    "model_dir = os.path.join('..', 'models', 'freight_predict')\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "if rf_model_freight is not None:\n",
    "    joblib.dump(rf_model_freight, os.path.join(model_dir, 'rf_regressor_freight.joblib'))\n",
    "    print(\"Modelo de frete salvo com sucesso!\")\n",
    "\n",
    "joblib.dump(scaler, os.path.join(model_dir, 'freight_scaler.joblib'))\n",
    "joblib.dump(one_hot_encoder, os.path.join(model_dir, 'freight_encoder.joblib'))\n",
    "print(\"Scaler e encoder salvos com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96bc1b6",
   "metadata": {},
   "source": [
    "### Obtendo um valor real para testar o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422653c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando modelo e dados para teste...\n",
      "Erro: Arquivo n√£o encontrado. Verifique se a c√©lula de treino foi executada: [Errno 2] No such file or directory: '..\\\\models\\\\freight_predict\\\\freight_prediction_dataset.csv'\n",
      "\n",
      "Preparando dados para a previs√£o...\n",
      "\n",
      "--- Extraindo e preparando um JSON para o teste da API ---\n",
      "Aqui est√£o os dados reais de uma amostra do seu conjunto de teste:\n",
      "Valor Real do Frete: R$ 12.65\n",
      "\n",
      "JSON de Exemplo para a Requisi√ß√£o POST:\n",
      "{\n",
      "    \"price\": 74.9,\n",
      "    \"product_weight_g\": 107.0,\n",
      "    \"product_volume_cm3\": 3211.0,\n",
      "    \"product_length_cm\": 19.0,\n",
      "    \"product_height_cm\": 13.0,\n",
      "    \"product_width_cm\": 13.0,\n",
      "    \"customer_state\": \"SP\",\n",
      "    \"seller_state\": \"PR\"\n",
      "}\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import json\n",
    "# --- 1. Carregando os Artefatos Salvos ---\n",
    "print(\"Carregando modelo e dados para teste...\")\n",
    "\n",
    "model_dir = os.path.join('..', 'models', 'freight_predict')\n",
    "scaler_path = os.path.join(model_dir, 'freight_scaler.joblib')\n",
    "encoder_path = os.path.join(model_dir, 'freight_encoder.joblib')\n",
    "data_path = os.path.join(model_dir, 'freight_prediction_dataset.csv')\n",
    "\n",
    "try:\n",
    "    loaded_scaler = joblib.load(scaler_path)\n",
    "    loaded_encoder = joblib.load(encoder_path)\n",
    "    df_freight_data = pd.read_csv(data_path)\n",
    "    print(\"Arquivos carregados com sucesso!\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Erro: Arquivo n√£o encontrado. Verifique se a c√©lula de treino foi executada: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. Preparando os Dados de Teste ---\n",
    "print(\"\\nPreparando dados para a previs√£o...\")\n",
    "\n",
    "numeric_features = [\n",
    "    'price', 'product_weight_g', 'product_volume_cm3', 'product_length_cm',\n",
    "    'product_height_cm', 'product_width_cm'\n",
    "]\n",
    "categorical_features = ['customer_state', 'seller_state']\n",
    "\n",
    "# Remove linhas com valores nulos nas features e no alvo\n",
    "all_features_and_target = numeric_features + categorical_features + ['freight_value']\n",
    "df_freight_data.dropna(subset=all_features_and_target, inplace=True)\n",
    "df_freight_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Cria as features e o alvo (em seus valores originais)\n",
    "X = df_freight_data[numeric_features + categorical_features]\n",
    "y = df_freight_data['freight_value']\n",
    "\n",
    "# Recria o MESMO conjunto de treino/teste usado no treino\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# --- 3. Extraindo e Revertendo uma Amostra Real ---\n",
    "print(\"\\n--- Extraindo e preparando um JSON para o teste da API ---\")\n",
    "\n",
    "# Escolhe a primeira amostra do conjunto de teste para a previs√£o\n",
    "sample_index = 0\n",
    "sample_data = X_test.iloc[sample_index].to_dict()\n",
    "sample_real_freight = y_test.iloc[sample_index]\n",
    "\n",
    "print(\"Aqui est√£o os dados reais de uma amostra do seu conjunto de teste:\")\n",
    "print(f\"Valor Real do Frete: R$ {sample_real_freight:.2f}\")\n",
    "\n",
    "# Exibe o JSON que voc√™ deve usar no seu arquivo de teste\n",
    "print(\"\\nJSON de Exemplo para a Requisi√ß√£o POST:\")\n",
    "json_data = json.dumps(sample_data, indent=4)\n",
    "print(json_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
